{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6UC0HP04/gWhP7RPCxsX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rayers-Ranjitkar/2461847_Rayers_Rental/blob/main/Worksheet_8_Ensemble_Methods_Hyperparameter_Tuning_Rayers_Ranjitkar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble Methods and Hyperparamter Tuning**"
      ],
      "metadata": {
        "id": "YA51deFP4M8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Implementing Classification Model**"
      ],
      "metadata": {
        "id": "4cIsep_74USE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhOotoA436jY",
        "outputId": "768f11c6-843a-4c8d-bbcc-f84eaf9f1808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree F1 Score: 0.9425\n",
            "Random Forest F1 Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# loading wine dataset\n",
        "wine = load_wine()\n",
        "\n",
        "# defining features and target\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42 #Random state fixes the shuffling so, that test and train dataset are the same\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# training decision tree classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42) #If two different split gives the same IG, we are always choosing the same split by passing random_state\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# predicting values\n",
        "dt_pred = dt_model.predict(X_test)\n",
        "\n",
        "# calculating f1 score\n",
        "dt_f1 = f1_score(y_test, dt_pred, average=\"macro\") #macro -> Simple average of all f1 score of all classes as the dataset is multiclass\n",
        "\n",
        "\n",
        "\n",
        "# training random forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# predicting values\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "# calculating f1 score\n",
        "rf_f1 = f1_score(y_test, rf_pred, average=\"macro\")\n",
        "\n",
        "\n",
        "\n",
        "# comparing f1 scores\n",
        "print(\"Decision Tree F1 Score:\", round(dt_f1, 4))\n",
        "print(\"Random Forest F1 Score:\", round(rf_f1, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree classified most samples correctly but having single tree can slighly overfit the model or miss complex pattern whereas  \n",
        "\n",
        "Random Forest classified all the samples correctly as combining many trees that\n",
        "too with random number of selected features and combining them at the last helps reducing the overfit and error caused by decision tree."
      ],
      "metadata": {
        "id": "tol6xTiS5Y7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "wiGU4LVm85RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Hyperparameter tuning**"
      ],
      "metadata": {
        "id": "nEnJ5ltx6N0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# creating base random forest\n",
        "rf_base = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# creating parameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200], # n_estimators = number of decision trees used in random forest #larger number = larger average = more stable\n",
        "    \"max_depth\": [None, 5, 10, 20], # None = no limit on the depth (keeps splitting until pure node)\n",
        "    \"min_samples_split\": [2, 5, 10] # min_sample_split -> no of rows in a node after split less than this bhayesi don't train as the model will memorize more train data and will overfit #It's a early stopping condition to prevent overfitting\n",
        "}\n",
        "\n",
        "# applying grid search\n",
        "grid = GridSearchCV(\n",
        "    estimator=rf_base, #estimator = the model that we want to tune\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1_macro\", #f1_macro = macro average of f1 score\n",
        "    cv=5, # Performing k-fold cross validation to reduce overfitting as model has to do well on all the folds and single average of 5 is calculated as output\n",
        ")\n",
        "\n",
        "# fitting grid search in the training data\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# printing best result\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best CV F1 Score:\", round(grid.best_score_, 4))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-hJMrM06NSO",
        "outputId": "1571c30b-0b65-4ed2-e68f-ebd992dd4408"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best CV F1 Score: 0.9788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>"
      ],
      "metadata": {
        "id": "k7St-vpx8-0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Implementing Regression Model**"
      ],
      "metadata": {
        "id": "0kgQfiP_7yCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# loading wine dataset\n",
        "wine = load_wine()\n",
        "\n",
        "print(wine.feature_names)\n",
        "print(wine.data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XcAeHDQ71km",
        "outputId": "84c6985a-a0e5-4436-f285-347038e70d05"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
            "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
            " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
            " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
            " ...\n",
            " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
            " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
            " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataframe\n",
        "df_wine = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "\n",
        "# creating regression target (output: label)\n",
        "y = df_wine[\"alcohol\"].values\n",
        "\n",
        "# creating input features dataset\n",
        "X = df_wine.drop(columns=[\"alcohol\"]).values\n",
        "\n",
        "# splitting dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# training decision tree regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "\n",
        "# predicting values\n",
        "dt_pred = dt_reg.predict(X_test)\n",
        "\n",
        "# checking r2 score\n",
        "dt_r2 = r2_score(y_test, dt_pred)\n",
        "\n",
        "print(\"Decision Tree R2 Score:\", round(dt_r2, 4))\n",
        "\n",
        "\n",
        "# training random forest regressor\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# predicting values\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "# checking r2 score\n",
        "rf_r2 = r2_score(y_test, rf_pred)\n",
        "\n",
        "print(\"Random Forest R2 Score:\", round(rf_r2, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuexj8slKKOL",
        "outputId": "62544254-8201-4efc-ec1f-77c8fa32c911"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree R2 Score: 0.4775\n",
            "Random Forest R2 Score: 0.7416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying 3 params for Random Forest Regression and performing hyperparamter tuning"
      ],
      "metadata": {
        "id": "WKFtPOd28bwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining parameter distribution\n",
        "param_dist = {\n",
        "    \"n_estimators\": [50, 100, 200, 300, 500],\n",
        "    \"max_depth\": [None, 5, 10, 20, 30],\n",
        "    \"min_samples_split\": [2, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# creating base model\n",
        "rf_base = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# applying randomized search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_base,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring=\"r2\", # Scoring = best metric to decide best paramters\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# fitting randomized search in traing data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# showing best parameters\n",
        "print(\"Best Params:\", random_search.best_params_)\n",
        "print(\"Best CV R2 Score:\", round(random_search.best_score_, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGm4wsq68PAS",
        "outputId": "36fdd589-563c-42f7-b4f4-2c72f65306fc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'n_estimators': 500, 'min_samples_split': 15, 'max_depth': 5}\n",
            "Best CV R2 Score: 0.5119\n"
          ]
        }
      ]
    }
  ]
}